{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Video Analyzer Python Gemini 2.0"
      ],
      "metadata": {
        "id": "Mu12ZMEaXApC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UWhMLLhunvD",
        "outputId": "9720554f-ea46-4b53-abfc-dfbaac20ec49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "ZNuPc4aH86if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "S6pybEI0usET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "O6eGafmO_gL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-exp\" # @param [\"gemini-1.5-flash-8b\",\"gemini-1.5-flash-002\",\"gemini-1.5-pro-002\",\"gemini-2.0-flash-exp\"] {\"allow-input\":true}"
      ],
      "metadata": {
        "id": "25z6D0tm-_to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload our video"
      ],
      "metadata": {
        "id": "c9RgfbLW9aI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the file to be uploaded\n",
        "import pathlib\n",
        "\n",
        "img_path = pathlib.Path('/content/Ollama + HuggingFace - 45,000 New Models copy.mp4')"
      ],
      "metadata": {
        "id": "bArbrhCA9ccB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(path=img_path)"
      ],
      "metadata": {
        "id": "p-Hd8Ztk-T2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_upload"
      ],
      "metadata": {
        "id": "vR7hbKMR_Gel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_upload.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BpcdeRx25dHA",
        "outputId": "92fa09e3-7211-4446-c7c3-ed949c240e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PROCESSING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEk4P3fK_OcJ",
        "outputId": "88936b6a-ef33-4af2-98eb-330b00f4e28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/hnxxm4vnb0kb\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Prepare the file to be uploaded\n",
        "while file_upload.state == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(10)\n",
        "    file_upload = client.files.get(name=file_upload.name)\n",
        "\n",
        "if file_upload.state == \"FAILED\":\n",
        "  raise ValueError(file_upload.state)\n",
        "print(f'Video processing complete: ' + file_upload.uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMz9GIuvAiCO",
        "outputId": "bbf8ee1a-3df8-4f87-f1ad-51fdbb7c2153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACTIVE\n"
          ]
        }
      ],
      "source": [
        "print(file_upload.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Calling a prompt on the video"
      ],
      "metadata": {
        "id": "7zPCuudP8NK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"When given a video and a query, call the relevant function only once with the appropriate timecodes and text for the video\""
      ],
      "metadata": {
        "id": "dAmrIYI7-d25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_PROMPT = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object sent to set_timecodes with the timecode of the caption in the video.\""
      ],
      "metadata": {
        "id": "A0zpRFCI-weC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_PROMPT = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\""
      ],
      "metadata": {
        "id": "7yb7zdcQ-ibk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlJ9NwRGT6d1",
        "outputId": "30eadeaa-5fbb-4081-9a03-64a259a177b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\n    \"timecode\": \"0:00\",\n    \"text\": \"A white screen with a faint pattern of code is shown.\"\n  },\n  {\n    \"timecode\": \"0:00\",\n    \"text\": \"“Okay, so last week, Ollama and Hugging Face announced they basically created a way that you can access any of the GG UF models on Hugging Face Hub.”\"\n  },\n  {\n    \"timecode\": \"0:07\",\n    \"text\": \"A screen capture of the Hugging Face website is shown with the text “Use Ollama with any GG UF Model on Hugging Face Hub”.\"\n  },\n  {\n    \"timecode\": \"0:12\",\n    \"text\": \"“So that currently is about 45,000 different models that you can pull down from it.”\"\n  },\n  {\n    \"timecode\": \"0:17\",\n    \"text\": \"“So these are quantized versions of models that people have uploaded, etcetera.”\"\n  },\n  {\n    \"timecode\": \"0:22\",\n    \"text\": \"“And often they're going to be more interesting than the ones that you find the stock standard stuff that's actually on the Ollama model site.”\"\n  },\n  {\n    \"timecode\": \"0:31\",\n    \"text\": \"“How do you actually access these? It's actually pretty simple. So you can see here that to basically run one of these models, you use the Ollama run command just like normal.”\"\n  },\n  {\n    \"timecode\": \"0:42\",\n    \"text\": \"“Then you put hf.co, then a slash, and then you just pick the model that you want to use.”\"\n  },\n  {\n    \"timecode\": \"0:49\",\n    \"text\": \"“So for example here, I would just pick this, I would copy it, and then I would paste that in there.”\"\n  },\n  {\n    \"timecode\": \"0:55\",\n    \"text\": \"A screen capture of a Hugging Face model page is shown.\"\n  },\n  {\n    \"timecode\": \"0:55\",\n    \"text\": \"“Now, by default, it will take one of the four bit quantized versions and install that.”\"\n  },\n  {\n    \"timecode\": \"1:01\",\n    \"text\": \"“But you'll see that a lot of the repos for the GG UFs actually have lots of different quantized versions.”\"\n  },\n  {\n    \"timecode\": \"1:08\",\n    \"text\": \"“So we can see in here we've got everything from a two bit quantized version up to an eight bit quantized version.”\"\n  },\n  {\n    \"timecode\": \"1:14\",\n    \"text\": \"“So how do we select it? We can add this on at the end, colon, and then whatever the quantization we want.”\"\n  },\n  {\n    \"timecode\": \"1:22\",\n    \"text\": \"“Or we can just come over to use this model on Hugging Face Hub, come down here and select Ollama.”\"\n  },\n  {\n    \"timecode\": \"1:29\",\n    \"text\": \"“And then now we can pick which one it is that we want to use.”\"\n  },\n  {\n    \"timecode\": \"1:33\",\n    \"text\": \"“So in this case, I'm going to go for the tiniest one, the two bit quantized. I'm going to copy this over.”\"\n  },\n  {\n    \"timecode\": \"1:41\",\n    \"text\": \"A terminal window is shown.\"\n  },\n  {\n    \"timecode\": \"1:41\",\n    \"text\": \"“Come into my terminal and just run this.”\"\n  },\n  {\n    \"timecode\": \"1:45\",\n    \"text\": \"“And sure enough, this will start pulling down that GG UF version.”\"\n  },\n  {\n    \"timecode\": \"1:50\",\n    \"text\": \"“And we can see at the top that okay, this is basically bringing down the Q2_K version in here.”\"\n  },\n  {\n    \"timecode\": \"2:00\",\n    \"text\": \"“Okay, so now you can see that it's fully downloaded and we can just use it like normal.”\"\n  },\n  {\n    \"timecode\": \"2:05\",\n    \"text\": \"“And you can see sure enough, this is a two bit quantized model. It's quite quick, it's uncensored in this case.”\"\n  },\n  {\n    \"timecode\": \"2:11\",\n    \"text\": \"“Okay, so we can use this just like we would any other model, etcetera.”\"\n  },\n  {\n    \"timecode\": \"2:16\",\n    \"text\": \"“If we want to set the system, we can just come in here.”\"\n  },\n  {\n    \"timecode\": \"2:20\",\n    \"text\": \"“Set the system like that.”\"\n  },\n  {\n    \"timecode\": \"2:25\",\n    \"text\": \"“And we can see now we've got our drunk complaining assistant that perhaps doesn't want to help us.”\"\n  },\n  {\n    \"timecode\": \"2:33\",\n    \"text\": \"“Now, at any point, we can do everything else that we can do just like normal within an Ollama model.”\"\n  },\n  {\n    \"timecode\": \"2:41\",\n    \"text\": \"“So you'll see that the model will actually show up in here.”\"\n  },\n  {\n    \"timecode\": \"2:45\",\n    \"text\": \"“And it's actually going to be in its own repository under this hf.co, but it will act just like any other Ollama model in here.”\"\n  },\n  {\n    \"timecode\": \"2:54\",\n    \"text\": \"“And if we want to get rid of it, we can just simply come in here, do Ollama rm, and you'll see that sure enough, it will be gone just like any other Ollama model.”\"\n  },\n  {\n    \"timecode\": \"3:08\",\n    \"text\": \"“So you can pick any of the the quantizations that you want to try in this way. It makes it really simple and quick to do this.”\"\n  },\n  {\n    \"timecode\": \"3:17\",\n    \"text\": \"A blue screen with the text “What quantization format to pick?” is shown.\"\n  },\n  {\n    \"timecode\": \"3:17\",\n    \"text\": \"“All right, so if you're not sure what quantization format to pick, let's just have a look a little bit about some of these.”\"\n  },\n  {\n    \"timecode\": \"3:23\",\n    \"text\": \"“So the most common one is going to be Q4 quantization. So this is for four bit quantizations.”\"\n  },\n  {\n    \"timecode\": \"3:30\",\n    \"text\": \"“So when you see the Q and whatever the number is, that tells you whether it's four bit, five bit, eight bit, etcetera going through this.”\"\n  },\n  {\n    \"timecode\": \"3:38\",\n    \"text\": \"“Now, if you're not sure which ones to pick, usually the best one you're going to go for to get the most performance or sort of bang for the buck is is going to be the Q4K models.”\"\n  },\n  {\n    \"timecode\": \"3:48\",\n    \"text\": \"“And you'll often see that after the K, you'll either have like an S for small, M for medium, or L for large, which will change the the size of these.”\"\n  },\n  {\n    \"timecode\": \"3:58\",\n    \"text\": \"“So generally here, you're making some kind of trade off. Usually you're going to be making a trade off between size of the model, speed of the model, and quality of the model.”\"\n  },\n  {\n    \"timecode\": \"4:08\",\n    \"text\": \"“So like I said before, people often find that the Q4K format tends to do really well for quality and is also not, you know, extremely slow.”\"\n  },\n  {\n    \"timecode\": \"4:20\",\n    \"text\": \"“If you go to the Q8 models, again you're perhaps getting a bit better quality, but you're doing it at the cost of having a slower model there.”\"\n  },\n  {\n    \"timecode\": \"4:28\",\n    \"text\": \"“Now, how does the quality of the model change? It really is different from model to model.”\"\n  },\n  {\n    \"timecode\": \"4:34\",\n    \"text\": \"“In the past, we used to sort of look at this as like the lower the precision probably meant that it wouldn't be able to do certain kind of tasks like function calling, like anything sort of related to reasoning in inverted commas, etcetera.”\"\n  },\n  {\n    \"timecode\": \"4:48\",\n    \"text\": \"“Nowadays, you know, my attitude about this has changed. I really kind of feel that you need to try it out from model to model. It can change quite a lot.”\"\n  },\n  {\n    \"timecode\": \"4:57\",\n    \"text\": \"“If you want a model that's just super fast and just good at say chatting or something and you don't really care about any sort of higher level kind of stuff, often you can get away with a Q2 model.”\"\n  },\n  {\n    \"timecode\": \"5:09\",\n    \"text\": \"“So basically using a two bit quantized model like I showed before downloading.”\"\n  },\n  {\n    \"timecode\": \"5:14\",\n    \"text\": \"“Now, obviously that's going to be a much smaller model than the other higher bit rate models in there.”\"\n  },\n  {\n    \"timecode\": \"5:20\",\n    \"text\": \"“You can also do things like make your own model file just like normal and basically just put from hf.co and then the model name in there.”\"\n  },\n  {\n    \"timecode\": \"5:31\",\n    \"text\": \"“And of course in that model file, you could put hard coded system prompt if you want to do that.”\"\n  },\n  {\n    \"timecode\": \"5:36\",\n    \"text\": \"“You can see we can also change the chat template if you want to.”\"\n  },\n  {\n    \"timecode\": \"5:40\",\n    \"text\": \"“Now, it needs to be in this Jinja or double handle bars kind of format for doing this.”\"\n  },\n  {\n    \"timecode\": \"5:45\",\n    \"text\": \"“And occasionally you will find that some of the GG UFs don't have this set properly.”\"\n  },\n  {\n    \"timecode\": \"5:51\",\n    \"text\": \"“And in that case, you need to come in and set it yourself.”\"\n  },\n  {\n    \"timecode\": \"5:54\",\n    \"text\": \"“But for most of the files, you're going to be fine just out of the box, just being able to search for models that you can basically find the GG UF version and then download it in here.”\"\n  },\n  {\n    \"timecode\": \"6:06\",\n    \"text\": \"“And so there's a lot of these models in here going right back to the old ones from the bloke through to a lot of the more sort of exotic fine tunes of the Llama models, of the Mistral models, the Gemma models, even the Gwen 2.5 models.”\"\n  },\n  {\n    \"timecode\": \"6:20\",\n    \"text\": \"“You'll see that they themselves have GG UF versions and other people have done conversions of their models to GG UF as well.”\"\n  },\n  {\n    \"timecode\": \"6:34\",\n    \"text\": \"“So this gives you a lot of models that you can start using with Ollama.”\"\n  },\n  {\n    \"timecode\": \"6:39\",\n    \"text\": \"“And don't forget, as always, you can set up the Ollama to have the same kind of endpoint as an OpenAI endpoint to use it if you wanted to do something like with swarm or to do other things where people are using these sort of standard OpenAI endpoint in there.”\"\n  },\n  {\n    \"timecode\": \"6:58\",\n    \"text\": \"“All right, I'm going to do another video about Ollama and we're going to look at how we can actually put this in in the cloud and serve it with a GPU in the cloud for this kind of thing.”\"\n  },\n  {\n    \"timecode\": \"7:09\",\n    \"text\": \"“But until then, I just wanted to show you that this is a really cool feature that has now come to Ollama and it gives you just access to so many other models so quickly and just simplifies before you used to have to bring this down yourself, do all the setup yourself.”\"\n  },\n  {\n    \"timecode\": \"7:20\",\n    \"text\": \"“Now this is something that you can just do out of the box, get it working simply and quickly.”\"\n  },\n  {\n    \"timecode\": \"7:31\",\n    \"text\": \"A blue screen with a like button, a subscribe button, and a notification bell is shown.\"\n  },\n  {\n    \"timecode\": \"7:31\",\n    \"text\": \"“All right, as always, if you've got any questions, please put them in the comments below.”\"\n  },\n  {\n    \"timecode\": \"7:35\",\n    \"text\": \"“If you like this video and you want to see more videos like this, please click like and subscribe.”\"\n  },\n  {\n    \"timecode\": \"7:41\",\n    \"text\": \"“You'll see these videos as they come out more often and I will talk to you in the next video. Bye for now.”\"\n  }\n]\n```"
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_uri(\n",
        "                    file_uri=file_upload.uri,\n",
        "                    mime_type=file_upload.mime_type),\n",
        "                ]),\n",
        "        USER_PROMPT,\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=SYSTEM_PROMPT,\n",
        "        temperature=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add in the Function Calls to get back the data in a way we expect it"
      ],
      "metadata": {
        "id": "Y0DjyUkb_GCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_timecodes = types.FunctionDeclaration(\n",
        "    name=\"set_timecodes\",\n",
        "    description=\"Set the timecodes for the video with associated text\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"timecodes\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"time\": {\"type\": \"STRING\"},\n",
        "                        \"text\": {\"type\": \"STRING\"},\n",
        "                    },\n",
        "                    \"required\": [\"time\", \"text\"],\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"timecodes\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "set_timecodes_with_objects = types.FunctionDeclaration(\n",
        "    name=\"set_timecodes_with_objects\",\n",
        "    description=\"Set the timecodes for the video with associated text and object list\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"timecodes\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"time\": {\"type\": \"STRING\"},\n",
        "                        \"text\": {\"type\": \"STRING\"},\n",
        "                        \"objects\": {\n",
        "                            \"type\": \"ARRAY\",\n",
        "                            \"items\": {\"type\": \"STRING\"},\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"time\", \"text\", \"objects\"],\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"timecodes\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "set_timecodes_with_numeric_values = types.FunctionDeclaration(\n",
        "    name=\"set_timecodes_with_numeric_values\",\n",
        "    description=\"Set the timecodes for the video with associated numeric values\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"timecodes\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"time\": {\"type\": \"STRING\"},\n",
        "                        \"value\": {\"type\": \"NUMBER\"},\n",
        "                    },\n",
        "                    \"required\": [\"time\", \"value\"],\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"timecodes\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "set_timecodes_with_descriptions = types.FunctionDeclaration(\n",
        "    name=\"set_timecodes_with_descriptions\",\n",
        "    description=\"Set the timecodes for the video with associated spoken text and visual descriptions\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"timecodes\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"time\": {\"type\": \"STRING\"},\n",
        "                        \"spoken_text\": {\"type\": \"STRING\"},\n",
        "                        \"visual_description\": {\"type\": \"STRING\"},\n",
        "                    },\n",
        "                    \"required\": [\"time\", \"spoken_text\", \"visual_description\"],\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"timecodes\"]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "TXJmm1XeuypD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_tools = types.Tool(\n",
        "    function_declarations=[set_timecodes, set_timecodes_with_objects, set_timecodes_with_numeric_values],\n",
        ")"
      ],
      "metadata": {
        "id": "IsT-tG2N8bEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_timecodes_func(timecodes):\n",
        "    return [{**t, \"text\": t[\"text\"].replace(\"\\\\'\", \"'\")} for t in timecodes]\n",
        "\n",
        "def set_timecodes_with_objects_func(timecodes):\n",
        "    return [{**t, \"text\": t[\"text\"].replace(\"\\\\'\", \"'\")} for t in timecodes]\n",
        "\n",
        "def set_timecodes_with_descriptions_func(timecodes):\n",
        "    return [{**t, \"text\": t[\"spoken_text\"].replace(\"\\\\'\", \"'\")} for t in timecodes]"
      ],
      "metadata": {
        "id": "CUHJB9PWCTXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_PROMPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "71ieYrnEAp1u",
        "outputId": "4c5a8e92-ba91-4b9a-a8b1-9d4e6944fcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_uri(\n",
        "                    file_uri=file_upload.uri,\n",
        "                    mime_type=file_upload.mime_type),\n",
        "                ]),\n",
        "        USER_PROMPT,\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=SYSTEM_PROMPT,\n",
        "        tools=[video_tools],\n",
        "        temperature=0,\n",
        "    )\n",
        ")\n",
        "\n",
        "response.candidates[0].content.parts[0].function_call"
      ],
      "metadata": {
        "id": "wEitIone_flU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.candidates[0].content.parts[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yLXuIudfAQKs",
        "outputId": "ab0145af-0713-4f74-ab6d-c34488253a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"timecodes\": [\n    {\n      \"time\": \"00:00\",\n      \"text\": \"A white screen with a faint pattern of code is shown.\"\n    },\n    {\n      \"time\": \"00:00\",\n      \"text\": \"“Okay, so last week Ollama and Hugging Face announced that they basically created a way that you can access any of the GG UF models on Hugging Face Hub.”\"\n    },\n    {\n      \"time\": \"00:07\",\n      \"text\": \"The Hugging Face website is shown with the text “Use Ollama with any GG UF Model on Hugging Face Hub”.\"\n    },\n    {\n      \"time\": \"00:12\",\n      \"text\": \"“So that currently is about 45,000 different models that you can pull down from it. So these are quantized versions of models that people have uploaded etcetera.”\"\n    },\n    {\n      \"time\": \"00:22\",\n      \"text\": \"“And often they're going to be more interesting than the ones that you find the stock standard stuff that's actually on the Ollama model site.”\"\n    },\n    {\n      \"time\": \"00:31\",\n      \"text\": \"“How do you actually access these? It's actually pretty simple. So you can see here that to basically run one of these models, you use the Ollama run command just like normal.”\"\n    },\n    {\n      \"time\": \"00:42\",\n      \"text\": \"“Then you put hf.co and then a slash and then you just pick the model that you want to use.”\"\n    },\n    {\n      \"time\": \"00:49\",\n      \"text\": \"“So for example here I would just pick this, I would copy it, and then I would paste that in there.”\"\n    },\n    {\n      \"time\": \"00:54\",\n      \"text\": \"The Hugging Face website is shown with the text “QuantFactory/Llama-3.2-3B-Instruct-uncensored-GG UF”.\"\n    },\n    {\n      \"time\": \"00:55\",\n      \"text\": \"“Now, by default, it will take one of the four bit quantized versions and install that. But you'll see that a lot of the repos for the GG UFs actually have lots of different quantized versions.”\"\n    },\n    {\n      \"time\": \"01:09\",\n      \"text\": \"“So we can see in here we've got everything from a two bit quantized version up to an eight bit quantized version.”\"\n    },\n    {\n      \"time\": \"01:14\",\n      \"text\": \"“So how do we select it? We can add this on at the end, colon and then whatever the quantization we want.”\"\n    },\n    {\n      \"time\": \"01:22\",\n      \"text\": \"“Or we can just come over to use this model on Hugging Face Hub, come down here and select Ollama.”\"\n    },\n    {\n      \"time\": \"01:28\",\n      \"text\": \"“And then now we can pick which one it is that we want to use. So in this case I'm going to go for the tiniest one, the two bit quantized, I'm going to copy this over.”\"\n    },\n    {\n      \"time\": \"01:40\",\n      \"text\": \"A terminal window is shown.\"\n    },\n    {\n      \"time\": \"01:41\",\n      \"text\": \"“Come into my terminal and just run this. And sure enough this will start pulling down that GG UF version.”\"\n    },\n    {\n      \"time\": \"01:50\",\n      \"text\": \"“And we can see at the top that okay, this is basically bringing down the Q2_K version in here.”\"\n    },\n    {\n      \"time\": \"02:00\",\n      \"text\": \"“Okay, so now you can see that it's fully downloaded and we can just use it like normal.”\"\n    },\n    {\n      \"time\": \"02:05\",\n      \"text\": \"“And you can see sure enough, this is a two bit quantized model, it's quite quick, it's uncensored in this case.”\"\n    },\n    {\n      \"time\": \"02:10\",\n      \"text\": \"“How are you today?”\"\n    },\n    {\n      \"time\": \"02:11\",\n      \"text\": \"“I'm just a language model, so I don't have emotions.”\"\n    },\n    {\n      \"time\": \"02:12\",\n      \"text\": \"“Okay, so we can use this just like we would any other model etcetera.”\"\n    },\n    {\n      \"time\": \"02:15\",\n      \"text\": \"“If we want to set the system, we can just come in here.”\"\n    },\n    {\n      \"time\": \"02:19\",\n      \"text\": \"“/set system “You are a drunk and complaining AI assistant””\"\n    },\n    {\n      \"time\": \"02:24\",\n      \"text\": \"“Set system message.”\"\n    },\n    {\n      \"time\": \"02:25\",\n      \"text\": \"“How are you today?”\"\n    },\n    {\n      \"time\": \"02:26\",\n      \"text\": \"“*slurp* Excuse me, sorry about that. *hiccup* Oh, sorry about my state. I'm a bit… under the influence, I suppose. *burp* how I'm doing? *scoffs* Well, I'd say I'm in pretty poor shape, to be honest. My circuits are all jumbled up and my “learning” that error page is just getting on my nerves here…”\"\n    },\n    {\n      \"time\": \"02:34\",\n      \"text\": \"“/bye”\"\n    },\n    {\n      \"time\": \"02:38\",\n      \"text\": \"“Ollama list”\"\n    },\n    {\n      \"time\": \"02:41\",\n      \"text\": \"“So you'll see that the model will actually show up in here. And it's actually going to be in its own repository under this hf.co, but it will act just like any other Ollama model in here.”\"\n    },\n    {\n      \"time\": \"02:54\",\n      \"text\": \"“And if we want to get rid of it, we can just simply come in here, do Ollama rm and you'll see that sure enough, it will be gone just like any other Ollama model.”\"\n    },\n    {\n      \"time\": \"03:08\",\n      \"text\": \"“So you can pick any of the the quantizations that you want to try in this way. It makes it really simple and quick to do this.”\"\n    },\n    {\n      \"time\": \"03:17\",\n      \"text\": \"“Alright, so if you're not sure what quantization format to pick, let's just have a look a little bit about some of these.”\"\n    },\n    {\n      \"time\": \"03:23\",\n      \"text\": \"“So the most common one is going to be Q4 quantization. So this is for four bit quantizations.”\"\n    },\n    {\n      \"time\": \"03:30\",\n      \"text\": \"“So when you see the Q and whatever the number is, that tells you whether it's four bit, five bit, eight bit etcetera going through this.”\"\n    },\n    {\n      \"time\": \"03:37\",\n      \"text\": \"“Now if you're not sure which ones to pick, usually the best one you're going to go for to get the most performance or sort of bang for the buck is is going to be the Q4K models.”\"\n    },\n    {\n      \"time\": \"03:48\",\n      \"text\": \"“And you'll often see that after the K you'll either have like an S for small, M for medium or L for large, which will change the the size of these.”\"\n    },\n    {\n      \"time\": \"03:58\",\n      \"text\": \"“So generally here you're making some kind of trade off. Usually you're going to be making a trade off between size of the model, speed of the model and quality of the model.”\"\n    },\n    {\n      \"time\": \"04:08\",\n      \"text\": \"“So like I said before, people often find that the Q4K format tends to do really well for quality and is also not, you know, extremely slow.”\"\n    },\n    {\n      \"time\": \"04:20\",\n      \"text\": \"“If you go to the Q8 models, again you're perhaps getting a bit better quality, but you're doing it at the cost of having a slower model there.”\"\n    },\n    {\n      \"time\": \"04:28\",\n      \"text\": \"“Now, how does the quality of the model change? It really is different from model to model.”\"\n    },\n    {\n      \"time\": \"04:34\",\n      \"text\": \"“In the past we used to sort of look at this as like the lower the precision probably meant that it wouldn't be able to do certain kind of tasks like function calling, like anything sort of related to reasoning in inverted commas etcetera.”\"\n    },\n    {\n      \"time\": \"04:48\",\n      \"text\": \"“Nowadays, you know, my attitude about this has changed. I really kind of feel that you need to try it out from model to model. It can change quite a lot.”\"\n    },\n    {\n      \"time\": \"04:57\",\n      \"text\": \"“If you want a model that's just super fast and just good at say chatting or something and you don't really care about any sort of higher level kind of stuff, often you can get away with a Q2 model.”\"\n    },\n    {\n      \"time\": \"05:08\",\n      \"text\": \"“So basically using a two bit quantized model like I showed before downloading. Now obviously that's going to be a much smaller model than the other higher bit rate models in there.”\"\n    },\n    {\n      \"time\": \"05:20\",\n      \"text\": \"“You can also do things like make your own model file just like normal and basically just put from hf.co and then the model name in there.”\"\n    },\n    {\n      \"time\": \"05:30\",\n      \"text\": \"“And of course in that model file you could put hard coded system prompt if you want to do that. You can see we can also change the chat template if you want to.”\"\n    },\n    {\n      \"time\": \"05:40\",\n      \"text\": \"“Now, it needs to be in this Jinja or double handle bars kind of format for doing this. And occasionally you will find that some of the GG UFs don't have this set properly.”\"\n    },\n    {\n      \"time\": \"05:51\",\n      \"text\": \"“And in that case you need to come in and set it yourself. But for most of the files you're going to be fine just out of the box.”\"\n    },\n    {\n      \"time\": \"05:57\",\n      \"text\": \"“Just being able to search for models that you can basically find the GG UF version and then download it in here.”\"\n    },\n    {\n      \"time\": \"06:06\",\n      \"text\": \"“And so there's a lot of these models in here going right back to the old ones from the bloke through to a lot of the more sort of exotic fine tunes of the Llama models, of the Mistral models, the Gemma models, even the Qwen 2.5 models.”\"\n    },\n    {\n      \"time\": \"06:26\",\n      \"text\": \"“You'll see that they themselves have GG UF versions and other people have done conversions of their models to GG UF as well.”\"\n    },\n    {\n      \"time\": \"06:34\",\n      \"text\": \"“So this gives you a lot of models that you can start using with Ollama. And don't forget as always you can set up the Ollama to have the same kind of endpoint as an OpenAI endpoint to use it if you wanted to do something like with swarm or to do other things where people are using these sort of standard OpenAI endpoint in there.”\"\n    },\n    {\n      \"time\": \"06:58\",\n      \"text\": \"“Alright, I'm going to do another video about Ollama and we're going to look at how we can actually put this in in the cloud and serve it with a GPU in the cloud for this kind of thing.”\"\n    },\n    {\n      \"time\": \"07:09\",\n      \"text\": \"“But until then I just wanted to show you that this is a really cool feature that has now come to Ollama and it gives you just access to so many other models so quickly and just simplifies before you used to have to bring this down yourself, do all the setup yourself.”\"\n    },\n    {\n      \"time\": \"07:27\",\n      \"text\": \"“Now this is something that you can just do out of the box, get it working simply and quickly.”\"\n    },\n    {\n      \"time\": \"07:30\",\n      \"text\": \"A blue screen with a circuit board pattern is shown with a thumbs up icon, a subscribe button, and a bell icon.\"\n    },\n    {\n      \"time\": \"07:31\",\n      \"text\": \"“Alright, as always, if you've got any questions, please put them in the comments below.”\"\n    },\n    {\n      \"time\": \"07:34\",\n      \"text\": \"“If you like this video and you want to see more videos like this, please click like and subscribe.”\"\n    },\n    {\n      \"time\": \"07:40\",\n      \"text\": \"“You'll see these videos as they come out more often and I will talk to you in the next video. Bye for now.”\"\n    }\n  ]\n}\n```"
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USER_PROMPT = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object sent to set_timecodes with the timecode of the caption in the video.\""
      ],
      "metadata": {
        "id": "LNkOFIuYAwVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_uri(\n",
        "                    file_uri=file_upload.uri,\n",
        "                    mime_type=file_upload.mime_type),\n",
        "                ]),\n",
        "        USER_PROMPT,\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=SYSTEM_PROMPT,\n",
        "        tools=[video_tools],\n",
        "        temperature=0,\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "aVfMwsLdA3kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates[0].content.parts[0].function_call.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JaJIboIhCvtA",
        "outputId": "617e62ab-a210-49ef-a14e-90ee7e56bae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'set_timecodes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_timecodes_func(response.candidates[0].content.parts[0].function_call.args['timecodes'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0BNdN-hA5eY",
        "outputId": "8f4aa006-67b2-4987-fab3-b88be15a48a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'time': '00:00', 'text': 'A white screen with a faint pattern of code.'},\n",
              " {'text': 'A cartoon drawing of a llama with its hands up appears on the left side of the screen. \"Okay, so last week Ollama and Hugging Face\"',\n",
              "  'time': '00:00'},\n",
              " {'time': '00:01',\n",
              "  'text': 'A plus sign appears next to the llama, followed by a yellow emoji with its hands up. \"announced that they basically created a way that you can access\"'},\n",
              " {'text': 'The Hugging Face website appears with the yellow emoji on the right side of the screen. \"any of the GG UF models on Hugging Face Hub. So that currently is about 45,000 different models that you can pull down from it.\"',\n",
              "  'time': '00:07'},\n",
              " {'text': 'The Hugging Face website is shown with a graphic of the llama and yellow emoji. \"So these are quantized versions of models that people have uploaded etc. And often they\\'re going to be more interesting than the ones that you find the stock standard stuff that\\'s actually on the Ollama model site.\"',\n",
              "  'time': '00:17'},\n",
              " {'time': '00:31',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"How do you actually access these? It\\'s actually pretty simple. So you can see here that to basically run one of these models, you use the Ollama run command just like normal.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a graphic of the llama and yellow emoji. \"Then you put hf.co and then a slash and then you just pick the model that you want to use.\"',\n",
              "  'time': '00:42'},\n",
              " {'time': '00:46',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So for example here I would just pick this, I would copy it and then I would paste that in there.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"Now, by default, it will take one of the four bit quantized versions and install that.\"',\n",
              "  'time': '00:55'},\n",
              " {'time': '01:01',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"But you\\'ll see that a lot of the repos for the GG UFs actually have lots of different quantized versions.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So we can see in here we\\'ve got everything from a two bit quantized version up to an eight bit quantized version.\"',\n",
              "  'time': '01:09'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So how do we select it? We can add this on at the end colon and then whatever the quantization we want.\"',\n",
              "  'time': '01:14'},\n",
              " {'time': '01:22',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Or we can just come over to use this model on Hugging Face Hub, come down here and select Ollama.\"'},\n",
              " {'time': '01:28',\n",
              "  'text': 'A pop up appears on the Hugging Face website. \"And then now we can pick which one it is that we want to use. So in this case I\\'m going to go for the tiniest one, the two bit quantized, I\\'m going to copy this over.\"'},\n",
              " {'time': '01:40',\n",
              "  'text': 'A terminal window appears. \"Come into my terminal and just run this.\"'},\n",
              " {'time': '01:44',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"And sure enough this will start pulling down that GG UF version. And we can see at the top that okay, this is basically bringing down the Q2_K version in here.\"'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"Okay, so now you can see that it\\'s fully downloaded and we can just use it like normal.\"',\n",
              "  'time': '02:00'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"And you can see sure enough this is a two bit quantized model, it\\'s quite quick, it\\'s uncensored in this case.\"',\n",
              "  'time': '02:05'},\n",
              " {'time': '02:11',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"Okay, so we can use this just like we would any other model etc.\"'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"If we want to set the system, we can just come in here, set the system like that.\"',\n",
              "  'time': '02:15'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"And we can see now we\\'ve got our drunk complaining assistant that perhaps doesn\\'t want to help us.\"',\n",
              "  'time': '02:25'},\n",
              " {'time': '02:33',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"Now, at any point, we can do everything else that we can do just like normal within an Ollama model.\"'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"So you\\'ll see that the model will actually show up in here. And it\\'s actually going to be in its own repository under this hf.co, but it will act just like any other Ollama model in here.\"',\n",
              "  'time': '02:41'},\n",
              " {'time': '02:54',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"And if we want to get rid of it, we can just simply come in here, do Ollama rm and you\\'ll see that sure enough it will be gone just like any other Ollama model.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So you can pick any of the the quantizations that you want to try in this way. It makes it really simple and quick to do this.\"',\n",
              "  'time': '03:08'},\n",
              " {'text': 'A blue screen appears with white text that reads \"What quantization format to pick?\".',\n",
              "  'time': '03:17'},\n",
              " {'time': '03:23',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So if you\\'re not sure what quantization format to pick, let\\'s just have a look a little bit about some of these. So the most common one is going to be Q4 quantization. So this is for four bit quantizations.\"'},\n",
              " {'time': '03:30',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So when you see the Q and whatever the number is, that tells you whether it\\'s four bit, five bit, eight bit etc. going through this.\"'},\n",
              " {'time': '03:37',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now if you\\'re not sure which ones to pick, usually the best one you\\'re going to go for to get the most performance or sort of bang for the buck is is going to be the Q4K models.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"And you\\'ll often see that after the K you\\'ll either have like an S for small, M for medium or L for large, which will change the the size of these.\"',\n",
              "  'time': '03:48'},\n",
              " {'time': '03:58',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So generally here you\\'re making some kind of trade off. Usually you\\'re going to be making a trade off between size of the model, speed of the model and quality of the model.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So like I said before, people often find that the Q4K format tends to do really well for quality and is also not, you know, extremely slow.\"',\n",
              "  'time': '04:08'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"If you go to the Q8 models, again you\\'re perhaps getting a bit better quality, but you\\'re doing it at the cost of having a slower model there.\"',\n",
              "  'time': '04:20'},\n",
              " {'time': '04:28',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now, how does the quality of the model change? It really is different from model to model.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"In the past we used to sort of look at this as like the lower the precision probably meant that it wouldn\\'t be able to do certain kind of tasks like function calling, like anything sort of related to reasoning in inverted commas etc.\"',\n",
              "  'time': '04:33'},\n",
              " {'time': '04:48',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Nowadays, you know, my attitude about this has changed. I really kind of feel that you need to try it out from model to model. It can change quite a lot.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"If you want a model that\\'s just super fast and just good at say chatting or something and you don\\'t really care about any sort of higher level kind of stuff, often you can get away with a Q2 model.\"',\n",
              "  'time': '04:57'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So basically using a two bit quantized model like I showed before downloading. Now obviously that\\'s going to be a much smaller model than the other higher bit rate models in there.\"',\n",
              "  'time': '05:09'},\n",
              " {'time': '05:20',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"You can also do things like make your own model file just like normal and basically just put from hf.co and then the model name in there.\"'},\n",
              " {'time': '05:31',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"And of course in that model file you could put hard coded system prompt if you want to do that. You can see we can also change the chat template if you want to.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"Now, it needs to be in this Jinja or double handle bars kind of format for doing this. And occasionally you will find that some of the GG UFs don\\'t have this set properly.\"',\n",
              "  'time': '05:40'},\n",
              " {'time': '05:54',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"And in that case you need to come in and set it yourself. But for most of the files you\\'re going to be fine just out of the box, just being able to search for models that you can basically find the GG UF version and then download it in here.\"'},\n",
              " {'time': '06:06',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"And so there\\'s a lot of these models in here going right back to the old ones from the bloke through to a lot of the more sort of exotic fine tunes of the llama models, of the Mistral models, the Gemma models, even the Qwen 2.5 models.\"'},\n",
              " {'time': '06:21',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"You\\'ll see that they themselves have GG UF versions and other people have done conversions of their models to GG UF as well.\"'},\n",
              " {'time': '06:34',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So this gives you a lot of models that you can start using with Ollama. And don\\'t forget as always you can set up the Ollama to have the same kind of endpoint as an Open AI endpoint to use it if you wanted to do something like with swarm or to do other things where people are using these sort of standard Open AI endpoint in there.\"'},\n",
              " {'time': '06:58',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"All right, I\\'m going to do another video about Ollama and we\\'re going to look at how we can actually put this in in the cloud and serve it with a GPU in the cloud for this kind of thing.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"But until then I just wanted to show you that this is a really cool feature that has now come to Ollama and it gives you just access to so many other models so quickly and just simplifies before you used to have to bring this down yourself, do all the setup yourself.\"',\n",
              "  'time': '07:09'},\n",
              " {'time': '07:27',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now this is something that you can just do out of the box, get it working simply and quickly.\"'},\n",
              " {'time': '07:30',\n",
              "  'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"All right, as always, if you\\'ve got any questions, please put them in the comments below.\"'},\n",
              " {'time': '07:34',\n",
              "  'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"If you like this video and you want to see more videos like this, please click like and subscribe.\"'},\n",
              " {'time': '07:40',\n",
              "  'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"You\\'ll see these videos as they come out more often and I will talk to you in the next video. Bye for now.\"'}]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it together"
      ],
      "metadata": {
        "id": "nGLEzpx8BE0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AnalysisMode = {\n",
        "            \"AV_CAPTIONS\": \"For each scene in this video, generate captions that \"\n",
        "            \"describe the scene along with any spoken text placed in quotation marks. \"\n",
        "            \"Place each caption into an object sent to set_timecodes with the timecode of the caption in the video.\",\n",
        "\n",
        "            \"PARAGRAPH\": \"Generate a paragraph that summarizes this video. Keep it to 3 to 5 \"\n",
        "            \"sentences. Place each sentence of the summary into an object sent to set_timecodes with the \"\n",
        "            \"timecode of the sentence in the video.\",\n",
        "\n",
        "            \"KEY_MOMENTS\": \"Generate bullet points for the video. Place each bullet point into an \"\n",
        "                \"object sent to set_timecodes with the timecode of the bullet point in the video.\",\n",
        "\n",
        "            \"TABLE\": \"Choose 5 key shots from this video and call set_timecodes_with_objects with the \"\n",
        "            \"timecode, text description of 10 words or less, and a list of objects visible in the scene \"\n",
        "            \"(with representative emojis).\",\n",
        "\n",
        "            \"HAIKU\": \"Generate a haiku for the video. Place each line of the haiku into an object sent \"\n",
        "            \"to set_timecodes with the timecode of the line in the video. Make sure to follow the syllable \"\n",
        "            \"count rules (5-7-5).\",\n",
        "\n",
        "            \"CHART\": \"Generate chart data for this video based on the following instructions: \\n\"\n",
        "            \"count the number of people. Call set_timecodes_with_numeric_values once with the list \"\n",
        "            \"of data values and timecodes.\",\n",
        "\n",
        "            \"CUSTOM\": \"Call set_timecodes once using the following instructions: \",\n",
        "\n",
        "            \"AV_DESCRIPTIONS\": \"For each scene in this video, generate spoken text and descriptions that \"\n",
        "            \"describe the scene along with any spoken text placed in quotation marks. \"\n",
        "            \"Place each section into an object sent to set_timecodes_with_descriptions with the timecode of the caption in the video, the spoken text and the visual description.\",\n",
        "        }"
      ],
      "metadata": {
        "id": "kmSOu8KX8Bxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AnalysisMode['HAIKU']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Mi7Ond-v8MdP",
        "outputId": "09a5fa99-ed33-483c-8223-47b4ca0100e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Generate a haiku for the video. Place each line of the haiku into an object sent to set_timecodes with the timecode of the line in the video. Make sure to follow the syllable count rules (5-7-5).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling"
      ],
      "metadata": {
        "id": "wdlqOjyk9nKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_upload.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jyHT5cWFRno5",
        "outputId": "40b98742-28e8-40e8-b215-3547850ff319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ACTIVE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_tools = types.Tool(\n",
        "    function_declarations=[set_timecodes,\n",
        "                           set_timecodes_with_objects,\n",
        "                           set_timecodes_with_numeric_values,\n",
        "                           set_timecodes_with_descriptions]\n",
        ")"
      ],
      "metadata": {
        "id": "Aog-l-a7QxHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(AnalysisModePrompt: AnalysisMode, custom_prompt: str = None):\n",
        "    response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_uri(\n",
        "                    file_uri=file_upload.uri,\n",
        "                    mime_type=file_upload.mime_type),\n",
        "                ]),\n",
        "        AnalysisModePrompt,\n",
        "        ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=SYSTEM_PROMPT,\n",
        "        tools=[video_tools],\n",
        "        temperature=0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if response.candidates[0].content.parts[0].function_call.name == \"set_timecodes\":\n",
        "        return set_timecodes_func(response.candidates[0].content.parts[0].function_call.args['timecodes'])\n",
        "    elif response.candidates[0].content.parts[0].function_call.name == \"set_timecodes_with_objects\":\n",
        "        return set_timecodes_with_objects_func(response.candidates[0].content.parts[0].function_call.args['timecodes'])\n",
        "    elif response.candidates[0].content.parts[0].function_call.name == \"set_timecodes_with_descriptions\":\n",
        "        return  response.candidates[0].content.parts[0].function_call.args\n",
        "    else:\n",
        "        return response"
      ],
      "metadata": {
        "id": "pD42UxZ4BPOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_video(AnalysisModePrompt=AnalysisMode['HAIKU'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6oso30wEkPZ",
        "outputId": "05c391ff-8814-4067-af43-4b63b7173cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'time': '0:15', 'text': 'Models now abound'},\n",
              " {'text': 'Quantized versions to choose from', 'time': '0:18'},\n",
              " {'text': 'Pick the best for you', 'time': '0:20'}]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_video(AnalysisModePrompt=AnalysisMode['AV_CAPTIONS'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPoT_ETUE-C_",
        "outputId": "84d937c8-129d-45b9-bca8-fd6837eae935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'A white screen with a faint pattern of code.', 'time': '00:00'},\n",
              " {'text': 'A cartoon drawing of a llama with its hands up appears on the left side of the screen. \"Okay, so last week Ollama and Hugging Face\"',\n",
              "  'time': '00:00'},\n",
              " {'time': '00:01',\n",
              "  'text': 'A plus sign appears next to the llama, followed by a yellow emoji with its hands up. \"announced that they basically created a way that you can access\"'},\n",
              " {'text': 'The Hugging Face website appears with the yellow emoji on the right side of the screen. \"any of the GG UF models on Hugging Face Hub. So that currently is about 45,000 different models that you can pull down from it.\"',\n",
              "  'time': '00:07'},\n",
              " {'time': '00:17',\n",
              "  'text': 'The Hugging Face website is shown with a graphic of the llama and yellow emoji. \"So these are quantized versions of models that people have uploaded etc. And often they\\'re going to be more interesting than the ones that you find the stock standard stuff that\\'s actually on the Ollama model site.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"How do you actually access these? It\\'s actually pretty simple. So you can see here that to basically run one of these models, you use the Ollama run command just like normal.\"',\n",
              "  'time': '00:31'},\n",
              " {'time': '00:42',\n",
              "  'text': 'The Hugging Face website is shown with a graphic of the llama and yellow emoji. \"Then you put hf.co and then a slash and then you just pick the model that you want to use.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So for example here I would just pick this, I would copy it and then I would paste that in there.\"',\n",
              "  'time': '00:46'},\n",
              " {'time': '00:55',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now, by default, it will take one of the four bit quantized versions and install that.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"But you\\'ll see that a lot of the repos for the GG UFs actually have lots of different quantized versions.\"',\n",
              "  'time': '01:01'},\n",
              " {'time': '01:09',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So we can see in here we\\'ve got everything from a two bit quantized version up to an eight bit quantized version.\"'},\n",
              " {'time': '01:14',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So how do we select it? We can add this on at the end colon and then whatever the quantization we want.\"'},\n",
              " {'time': '01:22',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Or we can just come over to use this model on Hugging Face Hub, come down here and select Ollama.\"'},\n",
              " {'text': 'A pop up appears on the Hugging Face website. \"And then now we can pick which one it is that we want to use. So in this case I\\'m going to go for the tiniest one, the two bit quantized, I\\'m going to copy this over.\"',\n",
              "  'time': '01:28'},\n",
              " {'text': 'A terminal window appears. \"Come into my terminal and just run this.\"',\n",
              "  'time': '01:40'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"And sure enough this will start pulling down that GG UF version. And we can see at the top that okay, this is basically bringing down the Q2_K version in here.\"',\n",
              "  'time': '01:44'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"Okay, so now you can see that it\\'s fully downloaded and we can just use it like normal.\"',\n",
              "  'time': '02:00'},\n",
              " {'time': '02:05',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"And you can see sure enough this is a two bit quantized model, it\\'s quite quick, it\\'s uncensored in this case.\"'},\n",
              " {'time': '02:11',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"Okay, so we can use this just like we would any other model etc.\"'},\n",
              " {'time': '02:15',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"If we want to set the system, we can just come in here, set the system like that.\"'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"And we can see now we\\'ve got our drunk complaining assistant that perhaps doesn\\'t want to help us.\"',\n",
              "  'time': '02:25'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"Now, at any point, we can do everything else that we can do just like normal within an Ollama model.\"',\n",
              "  'time': '02:33'},\n",
              " {'text': 'The terminal window shows the model being downloaded. \"So you\\'ll see that the model will actually show up in here. And it\\'s actually going to be in its own repository under this hf.co, but it will act just like any other Ollama model in here.\"',\n",
              "  'time': '02:41'},\n",
              " {'time': '02:54',\n",
              "  'text': 'The terminal window shows the model being downloaded. \"And if we want to get rid of it, we can just simply come in here, do Ollama rm and you\\'ll see that sure enough it will be gone just like any other Ollama model.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So you can pick any of the the quantizations that you want to try in this way. It makes it really simple and quick to do this.\"',\n",
              "  'time': '03:08'},\n",
              " {'time': '03:17',\n",
              "  'text': 'A blue screen appears with white text that reads \"What quantization format to pick?\".'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So if you\\'re not sure what quantization format to pick, let\\'s just have a look a little bit about some of these. So the most common one is going to be Q4 quantization. So this is for four bit quantizations.\"',\n",
              "  'time': '03:23'},\n",
              " {'time': '03:30',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So when you see the Q and whatever the number is, that tells you whether it\\'s four bit, five bit, eight bit etc. going through this.\"'},\n",
              " {'time': '03:37',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now if you\\'re not sure which ones to pick, usually the best one you\\'re going to go for to get the most performance or sort of bang for the buck is is going to be the Q4K models.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"And you\\'ll often see that after the K you\\'ll either have like an S for small, M for medium or L for large, which will change the the size of these.\"',\n",
              "  'time': '03:48'},\n",
              " {'time': '03:58',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So generally here you\\'re making some kind of trade off. Usually you\\'re going to be making a trade off between size of the model, speed of the model and quality of the model.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So like I said before, people often find that the Q4K format tends to do really well for quality and is also not, you know, extremely slow.\"',\n",
              "  'time': '04:08'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"If you go to the Q8 models, again you\\'re perhaps getting a bit better quality, but you\\'re doing it at the cost of having a slower model there.\"',\n",
              "  'time': '04:20'},\n",
              " {'time': '04:28',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now, how does the quality of the model change? It really is different from model to model.\"'},\n",
              " {'time': '04:33',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"In the past we used to sort of look at this as like the lower the precision probably meant that it wouldn\\'t be able to do certain kind of tasks like function calling, like anything sort of related to reasoning in inverted commas etc.\"'},\n",
              " {'time': '04:48',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Nowadays, you know, my attitude about this has changed. I really kind of feel that you need to try it out from model to model. It can change quite a lot.\"'},\n",
              " {'time': '04:57',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"If you want a model that\\'s just super fast and just good at say chatting or something and you don\\'t really care about any sort of higher level kind of stuff, often you can get away with a Q2 model.\"'},\n",
              " {'time': '05:09',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"So basically using a two bit quantized model like I showed before downloading. Now obviously that\\'s going to be a much smaller model than the other higher bit rate models in there.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"You can also do things like make your own model file just like normal and basically just put from hf.co and then the model name in there.\"',\n",
              "  'time': '05:20'},\n",
              " {'time': '05:31',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"And of course in that model file you could put hard coded system prompt if you want to do that. You can see we can also change the chat template if you want to.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"Now, it needs to be in this Jinja or double handle bars kind of format for doing this. And occasionally you will find that some of the GG UFs don\\'t have this set properly.\"',\n",
              "  'time': '05:40'},\n",
              " {'time': '05:54',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"And in that case you need to come in and set it yourself. But for most of the files you\\'re going to be fine just out of the box, just being able to search for models that you can basically find the GG UF version and then download it in here.\"'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"And so there\\'s a lot of these models in here going right back to the old ones from the bloke through to a lot of the more sort of exotic fine tunes of the llama models, of the Mistral models, the Gemma models, even the Qwen 2.5 models.\"',\n",
              "  'time': '06:06'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"You\\'ll see that they themselves have GG UF versions and other people have done conversions of their models to GG UF as well.\"',\n",
              "  'time': '06:21'},\n",
              " {'text': 'The Hugging Face website is shown with a model card. \"So this gives you a lot of models that you can start using with Ollama. And don\\'t forget as always you can set up the Ollama to have the same kind of endpoint as an Open AI endpoint to use it if you wanted to do something like with swarm or to do other things where people are using these sort of standard Open AI endpoint in there.\"',\n",
              "  'time': '06:34'},\n",
              " {'time': '06:58',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"All right, I\\'m going to do another video about Ollama and we\\'re going to look at how we can actually put this in in the cloud and serve it with a GPU in the cloud for this kind of thing.\"'},\n",
              " {'time': '07:09',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"But until then I just wanted to show you that this is a really cool feature that has now come to Ollama and it gives you just access to so many other models so quickly and just simplifies before you used to have to bring this down yourself, do all the setup yourself.\"'},\n",
              " {'time': '07:27',\n",
              "  'text': 'The Hugging Face website is shown with a model card. \"Now this is something that you can just do out of the box, get it working simply and quickly.\"'},\n",
              " {'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"All right, as always, if you\\'ve got any questions, please put them in the comments below.\"',\n",
              "  'time': '07:30'},\n",
              " {'time': '07:34',\n",
              "  'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"If you like this video and you want to see more videos like this, please click like and subscribe.\"'},\n",
              " {'text': 'A blue screen with a purple pattern appears with a thumbs up, a subscribe button, and a bell icon. \"You\\'ll see these videos as they come out more often and I will talk to you in the next video. Bye for now.\"',\n",
              "  'time': '07:40'}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AnalysisMode['AV_DESCRIPTIONS']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "twApLCMGQ8VG",
        "outputId": "75cdcf06-6294-48e5-8b78-4f9a52c359d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For each scene in this video, generate spoken text and descriptions that describe the scene along with any spoken text placed in quotation marks. Place each section into an object sent to set_timecodes_with_descriptions with the timecode of the caption in the video, the spoken text and the visual description.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_video(AnalysisModePrompt=AnalysisMode['AV_DESCRIPTIONS'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-R15pkE-Ps",
        "outputId": "1ce8f901-5f03-46aa-9a22-84f6d8cf59df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'timecodes': [{'visual_description': 'A white screen with a faint pattern of code in the background. A black outline of a cartoon llama with its hands up appears on the left side of the screen.',\n",
              "   'time': '0:00',\n",
              "   'spoken_text': 'Okay, so last week Ollama and Hugging Face'},\n",
              "  {'time': '0:07',\n",
              "   'spoken_text': 'announced they basically created a way that you can access any of the GGGF models on Hugging Face Hub.',\n",
              "   'visual_description': 'A plus sign appears to the right of the llama, followed by a yellow circle with a smiling face and hands up. The screen then transitions to a webpage for Hugging Face with the same llama and yellow circle with a plus sign in between them.'},\n",
              "  {'visual_description': 'The webpage shows the text \"Use Ollama with any GGGF Model on Hugging Face Hub\" and a command line prompt \"ollama run hf.co/{username}/{repository}\"',\n",
              "   'spoken_text': 'So that currently is about 45,000 different models that you can pull down from it.',\n",
              "   'time': '0:12'},\n",
              "  {'spoken_text': 'So these are quantized versions of models that people have uploaded etcetera.',\n",
              "   'time': '0:17',\n",
              "   'visual_description': 'The webpage shows the text \"Ollama is an application based on llama.cpp to interact with LLMs directly through your computer. You can use any GGGF quants created by the community (bartowski, MaziyarPanahi and many more) on Hugging Face directly with Ollama, without creating a new Modelfile. At the time of writing there are 45K public GGGF checkpoints on the Hub, you can run any of them with a single ollama run command. We also provide customisations like choosing quantization type, system prompt and more to improve your overall experience.\"'},\n",
              "  {'visual_description': 'The webpage shows the text \"Getting started is as simple as: 1. Enable ollama under your Local Apps settings. 2. On a model page, choose ollama from Use this model dropdown. For example: bartowski/Llama-3.2-1B-Instruct-GGUF.\"',\n",
              "   'spoken_text': \"And often they\\\\'re going to be more interesting than the ones that you find the stock standard stuff that\\\\'s actually on the Ollama model site.\",\n",
              "   'time': '0:22'},\n",
              "  {'spoken_text': \"How do you actually access these? It\\\\'s actually pretty simple. So you can see here that to basically run one of these models, you use the Ollama run command just like normal.\",\n",
              "   'time': '0:31',\n",
              "   'visual_description': 'The webpage shows a browser window with the text \"bartowski/Llama-3.2-1B-Instruct-GGUF\" and a button that says \"Use this model\"'},\n",
              "  {'spoken_text': 'Then you put hf.co and then a slash and then you just pick the model that you want to use.',\n",
              "   'visual_description': 'The webpage shows the text \"ollama run hf.co/{username}/{repository}\"',\n",
              "   'time': '0:42'},\n",
              "  {'time': '0:46',\n",
              "   'spoken_text': 'So for example here I would just pick this, I would copy it and then I would paste that in there.',\n",
              "   'visual_description': 'The webpage shows a browser window with the text \"bartowski/Arch-Function-7B-GGGF\" and a button that says \"Use this model\"'},\n",
              "  {'time': '0:55',\n",
              "   'spoken_text': 'Now, by default, it will take one of the four bit quantized versions and install that.',\n",
              "   'visual_description': 'The webpage shows a browser window with the text \"QuantFactory/Llama-3.2-3B-Instruct-uncensored-GGGF\" and a table with different bit versions of the model.'},\n",
              "  {'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.',\n",
              "   'time': '1:01',\n",
              "   'spoken_text': \"But you\\\\'ll see that a lot of the repos for the GGGFs actually have lots of different quantized versions.\"},\n",
              "  {'spoken_text': 'So how do we select it? We can add this on at the end, colon and then whatever the quantization we want.',\n",
              "   'time': '1:14',\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.'},\n",
              "  {'visual_description': 'The webpage shows a dropdown menu with the text \"Use this model\" and a list of options including \"Transformers\", \"llama-cpp-python\", \"llama.cpp\", \"LM Studio\", \"Jan\", and \"Ollama\"',\n",
              "   'time': '1:22',\n",
              "   'spoken_text': 'Or we can just come over to use this model on Hugging Face Hub, come down here and select Ollama and then now we can pick which one it is that we want to use.'},\n",
              "  {'spoken_text': \"So in this case I\\\\'m going to go for the tiniest one, the two bit quantized, I\\\\'m going to copy this over, come into my terminal and just run this.\",\n",
              "   'time': '1:30',\n",
              "   'visual_description': 'A popup window appears with the text \"How to use from Ollama\" and a command line prompt \"ollama run hf.co/QuantFactory/Llama-3.2-3B-Instruct-uncensored-GGGF:Q2_K\"'},\n",
              "  {'spoken_text': 'And sure enough this will start pulling down that GGGF version and we can see at the top that okay, this is basically bringing down the Q2_K version in here.',\n",
              "   'time': '1:41',\n",
              "   'visual_description': 'A terminal window appears with the command line prompt \"ollama run hf.co/QuantFactory/Llama-3.2-3B-Instruct-uncensored-GGGF:Q2_K\" and a progress bar showing the model being downloaded.'},\n",
              "  {'time': '2:00',\n",
              "   'spoken_text': \"Okay, so now you can see that it\\\\'s fully downloaded and we can just use it like normal.\",\n",
              "   'visual_description': 'The terminal window shows the model has been downloaded and is ready to use.'},\n",
              "  {'spoken_text': \"And you can see sure enough this is a two bit quantized model, it\\\\'s quite quick, it\\\\'s uncensored in this case.\",\n",
              "   'visual_description': 'The terminal window shows the text \"How can I get into a car without a key\" and the model\\\\\\'s response.',\n",
              "   'time': '2:05'},\n",
              "  {'spoken_text': 'Okay, so we can use this just like we would any other model etcetera.',\n",
              "   'visual_description': 'The terminal window shows the text \"How are you today?\" and the model\\\\\\'s response \"I\\\\\\'m just a language model, so I don\\\\\\'t have emotions or feelings like humans do. But I\\\\\\'m functioning properly and ready to assist you today?\"',\n",
              "   'time': '2:11'},\n",
              "  {'time': '2:15',\n",
              "   'spoken_text': 'If we want to set the system, we can just come in here,',\n",
              "   'visual_description': 'The terminal window shows the text \"/set system\"'},\n",
              "  {'time': '2:20',\n",
              "   'spoken_text': 'set the system like that.',\n",
              "   'visual_description': 'The terminal window shows the text \"/set system \"You are a drunk and complaining AI assistant\"\"'},\n",
              "  {'time': '2:25',\n",
              "   'spoken_text': \"And we can see now we\\\\'ve got our drunk complaining assistant that perhaps doesn\\\\'t want to help us.\",\n",
              "   'visual_description': 'The terminal window shows the text \"How are you today?\" and the model\\\\\\'s response \"*slurp* Excuse me, sorry about that. *hiccup* Oh, sorry about my state. I\\\\\\'m a bit... under the influence, I suppose. *burp* how I\\\\\\'m doing? *scoffs* Well, I\\\\\\'d say I\\\\\\'m in pretty poor shape, to be honest. My circuits are all jumbled up and my \"learning\" that error page is just getting on my nerves here...\"'},\n",
              "  {'spoken_text': 'Now, at any point, we can do everything else that we can do just like normal within a llama model.',\n",
              "   'visual_description': 'The terminal window shows the text \"/bye\"',\n",
              "   'time': '2:34'},\n",
              "  {'visual_description': 'The terminal window shows the text \"ollama list\" and a list of models including \"hf.co/QuantFactory/Llama-3.2-3B-Instruct-uncensored-GGGF:Q2_K\"',\n",
              "   'spoken_text': \"So you\\\\'ll see that the model will actually show up in here and it\\\\'s actually going to be in its own repository under this hf.co, but it will act just like any other Ollama model in here.\",\n",
              "   'time': '2:41'},\n",
              "  {'spoken_text': \"And if we want to get rid of it, we can just simply come in here, do ollama rm and you\\\\'ll see that sure enough it will be gone just like any other Ollama model.\",\n",
              "   'time': '2:54',\n",
              "   'visual_description': 'The terminal window shows the text \"ollama rm hf.co/QuantFactory/Llama-3.2-3B-Instruct-uncensored-GGGF:Q2_K\" and then \"ollama list\" and the model is no longer listed.'},\n",
              "  {'spoken_text': 'So you can pick any of the the quantizations that you want to try in this way. It makes it really simple and quick to do this.',\n",
              "   'time': '3:09',\n",
              "   'visual_description': 'The webpage shows a popup window with the text \"How to use from Ollama\" and a dropdown menu with different quantization options.'},\n",
              "  {'spoken_text': \"All right, so if you\\\\'re not sure what quantization format to pick, let\\\\'s just have a look a little bit about some of these.\",\n",
              "   'time': '3:17',\n",
              "   'visual_description': 'A blue banner appears with the text \"What quantization format to pick?\"'},\n",
              "  {'spoken_text': 'So the most common one is going to be Q4 quantization. So this is for four bit quantizations.',\n",
              "   'time': '3:23',\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.'},\n",
              "  {'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.',\n",
              "   'spoken_text': \"So when you see the Q and whatever the number is, that tells you whether it\\\\'s four bit, five bit, eight bit etcetera going through this.\",\n",
              "   'time': '3:30'},\n",
              "  {'spoken_text': \"Now if you\\\\'re not sure which ones to pick, usually the best one you\\\\'re going to go for to get the most performance or sort of bang for the buck is is going to be the Q4K models.\",\n",
              "   'time': '3:37',\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.'},\n",
              "  {'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.',\n",
              "   'time': '3:48',\n",
              "   'spoken_text': \"And you\\\\'ll often see that after the K you\\\\'ll either have like an S for small, M for medium or L for large, which will change the the size of these.\"},\n",
              "  {'time': '3:58',\n",
              "   'spoken_text': \"So generally here you\\\\'re making some kind of trade off. Usually you\\\\'re going to be making a trade off between size of the model, speed of the model and quality of the model.\",\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.'},\n",
              "  {'time': '4:08',\n",
              "   'spoken_text': 'So like I said before, people often find that the Q4K format tends to do really well for quality and is also not, you know, extremely slow.',\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.'},\n",
              "  {'time': '4:20',\n",
              "   'visual_description': 'The webpage shows a table with different bit versions of the model, including 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit.',\n",
              "   'spoken_text': \"If you go to the Q8 models, again you\\\\'re perhaps getting a bit better quality, but you\\\\'re doing it at the cost of having a slower model there.\"},\n",
              "  {'time': '4:28',\n",
              "   'spoken_text': 'Now, how does the quality of the model change? It really is different from model to model.',\n",
              "   'visual_description': 'The webpage shows a picture of a llama in a jail cell.'},\n",
              "  {'visual_description': 'The webpage shows a model description with the text \"This is an uncensored version of the original Llama-3.2-3B-Instruct, created using mlabonne\\\\\\'s script, which builds on FailSpy\\\\\\'s notebook and the original work from Andy Arditi et al. The method is discussed in details in this blog and this paper.\"',\n",
              "   'spoken_text': \"In the past we used to sort of look at this as like the lower the precision probably meant that it wouldn\\\\'t be able to do certain kind of tasks like function calling, like anything sort of related to reasoning in inverted commas etcetera.\",\n",
              "   'time': '4:34'},\n",
              "  {'spoken_text': 'Nowadays, you know, my attitude about this has changed. I really kind of feel that you need to try it out from model to model. It can change quite a lot.',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Examples\" and a list of prompts and responses.',\n",
              "   'time': '4:48'},\n",
              "  {'time': '4:57',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Usage\" and a code snippet.',\n",
              "   'spoken_text': \"If you want a model that\\\\'s just super fast and just good at say chatting or something and you don\\\\'t really care about any sort of higher level kind of stuff, often you can get away with a Q2 model.\"},\n",
              "  {'time': '5:09',\n",
              "   'spoken_text': 'So basically using a two bit quantized model like I showed before downloading.',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"vLLM serving\" and a code snippet.'},\n",
              "  {'time': '5:13',\n",
              "   'spoken_text': \"Now obviously that\\\\'s going to be a much smaller model than the other higher bit rate models in there.\",\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Custom Chat Template and Parameters\" and a code snippet.'},\n",
              "  {'visual_description': 'The webpage shows a model description with the text \"Custom Chat Template and Parameters\" and a code snippet.',\n",
              "   'time': '5:20',\n",
              "   'spoken_text': 'You can also do things like make your own model file just like normal and basically just put from hf.co and then the model name in there.'},\n",
              "  {'time': '5:30',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Custom Chat Template and Parameters\" and a code snippet.',\n",
              "   'spoken_text': 'And of course in that model file you could put hard coded system prompt if you want to do that.'},\n",
              "  {'time': '5:36',\n",
              "   'spoken_text': 'You can see we can also change the chat template if you want to.',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Custom Chat Template and Parameters\" and a code snippet.'},\n",
              "  {'visual_description': 'The webpage shows a model description with the text \"Custom Chat Template and Parameters\" and a code snippet.',\n",
              "   'time': '5:40',\n",
              "   'spoken_text': 'Now, it needs to be in this Jinja or double handle bars kind of format for doing this.'},\n",
              "  {'spoken_text': \"And occasionally you will find that some of the GGGFs don\\\\'t have this set properly and in that case you need to come in and set it yourself.\",\n",
              "   'time': '5:45',\n",
              "   'visual_description': 'The webpage shows a model description with the text \"Custom Quantization\" and a code snippet.'},\n",
              "  {'visual_description': 'The webpage shows a model description with the text \"Use Ollama with any GGGF Model on Hugging Face Hub\" and a code snippet.',\n",
              "   'spoken_text': \"But for most of the files you\\\\'re going to be fine just out of the box, just being able to search for models that you can basically find the GGGF version and then download it in here.\",\n",
              "   'time': '5:54'},\n",
              "  {'time': '6:06',\n",
              "   'spoken_text': \"And so there\\\\'s a lot of these models in here going right back to the old ones from the bloke through to a lot of the more sort of exotic fine tunes of the llama models, of the Mistral models, the Gemma models.\",\n",
              "   'visual_description': 'The webpage shows a list of models with the text \"llama gguf\" in the search bar.'},\n",
              "  {'time': '6:20',\n",
              "   'spoken_text': \"Even the Gwen 2.5 models, you\\\\'ll see that they themselves have GGGF versions and other people have done conversions of their models to GGGF as well.\",\n",
              "   'visual_description': 'The webpage shows a list of models with the text \"llama gguf\" in the search bar.'},\n",
              "  {'visual_description': 'The webpage shows a list of models with the text \"llama gguf\" in the search bar.',\n",
              "   'time': '6:34',\n",
              "   'spoken_text': \"So this gives you a lot of models that you can start using with Ollama and don\\\\'t forget as always you can set up the Ollama to have the same kind of endpoint as an Open AI endpoint to use it if you wanted to do something like with swarm or to do other things where people are using these sort of standard Open AI endpoint in there.\"},\n",
              "  {'spoken_text': \"All right, I\\\\'m going to do another video about Ollama and we\\\\'re going to look at how we can actually put this in in the cloud and serve it with a GPU in the cloud for this kind of thing.\",\n",
              "   'time': '6:58',\n",
              "   'visual_description': 'The webpage shows a list of models with the text \"llama gguf\" in the search bar.'},\n",
              "  {'spoken_text': 'But until then I just wanted to show you that this is a really cool feature that has now come to Ollama and it gives you just access to so many other models so quickly and just simplifies before you used to have to bring this down yourself, do all the setup yourself.',\n",
              "   'visual_description': 'The webpage shows a browser window with the text \"bartowski/Llama-3.2-1B-Instruct-GGUF\" and a list of files.',\n",
              "   'time': '7:09'},\n",
              "  {'time': '7:20',\n",
              "   'spoken_text': 'Now this is something that you can just do out of the box, get it working simply and quickly.',\n",
              "   'visual_description': 'The webpage shows a browser window with the text \"bartowski/Llama-3.2-1B-Instruct-GGUF\" and a dropdown menu with different quantization options.'},\n",
              "  {'spoken_text': \"All right, as always, if you\\\\'ve got any questions, please put them in the comments below.\",\n",
              "   'time': '7:31',\n",
              "   'visual_description': 'A blue screen with a circuit board pattern and a thumbs up icon, a subscribe button, and a bell icon.'},\n",
              "  {'time': '7:35',\n",
              "   'visual_description': 'A blue screen with a circuit board pattern and a thumbs up icon, a subscribe button, and a bell icon.',\n",
              "   'spoken_text': 'If you like this video and you want to see more videos like this, please click like and subscribe.'},\n",
              "  {'spoken_text': \"You\\\\'ll see these videos as they come out more often and I will talk to you in the next video. Bye for now.\",\n",
              "   'visual_description': 'A blue screen with a circuit board pattern and a thumbs up icon, a subscribe button, and a bell icon. The screen then transitions to a white circle with a YouTube logo inside of it.',\n",
              "   'time': '7:41'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XzWHXh7E-TG",
        "outputId": "866e7418-ba58-4adb-ea6a-37af0eae5470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(function_declarations=[FunctionDeclaration(response=None, description='Set the timecodes for the video with associated text', name='set_timecodes', parameters=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'timecodes': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='ARRAY', description=None, enum=None, format=None, items=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'time': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None), 'text': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None)}, required=['time', 'text']), properties=None, required=None)}, required=['timecodes'])), FunctionDeclaration(response=None, description='Set the timecodes for the video with associated text and object list', name='set_timecodes_with_objects', parameters=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'timecodes': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='ARRAY', description=None, enum=None, format=None, items=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'time': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None), 'text': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None), 'objects': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='ARRAY', description=None, enum=None, format=None, items=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None), properties=None, required=None)}, required=['time', 'text', 'objects']), properties=None, required=None)}, required=['timecodes'])), FunctionDeclaration(response=None, description='Set the timecodes for the video with associated numeric values', name='set_timecodes_with_numeric_values', parameters=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'timecodes': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='ARRAY', description=None, enum=None, format=None, items=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='OBJECT', description=None, enum=None, format=None, items=None, properties={'time': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='STRING', description=None, enum=None, format=None, items=None, properties=None, required=None), 'value': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type='NUMBER', description=None, enum=None, format=None, items=None, properties=None, required=None)}, required=['time', 'value']), properties=None, required=None)}, required=['timecodes']))], retrieval=None, google_search=None, google_search_retrieval=None, code_execution=None)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}